{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c52a1483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.3.0-py3-none-any.whl (981 kB)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.21.0-py3-none-any.whl (358 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting async-generator>=1.9\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: outcome, h11, async-generator, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed async-generator-1.10 h11-0.13.0 outcome-1.2.0 selenium-4.3.0 trio-0.21.0 trio-websocket-0.9.2 wsproto-1.1.0\n"
     ]
    }
   ],
   "source": [
    "## importing the libraries:\n",
    "!pip install selenium\n",
    "\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import re\n",
    "\n",
    "## importing require exceptions:-\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException,WebDriverException \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e3a1c4",
   "metadata": {},
   "source": [
    "# flipkart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "382bda07",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r'C:\\Users\\USER\\Downloads\\chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f849879",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fetching the websites\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "time.sleep(2)\n",
    "\n",
    "Item_List=['laptops','Smartphones','smart watches' ,'Computer moniters', \n",
    "                 'Digital Watches','Cameras','Trimmers','Keyboards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ba51271",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=[]\n",
    "full_reviews=[]\n",
    "ratings=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1b65039",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3076475175.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [22]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Find the search bar\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "product_urls=[]\n",
    "for i in Item_List:\n",
    "    Find the search bar\n",
    "    search_button = driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']/input\")\n",
    "    search_button.clear()\n",
    "    search_button.send_keys(i)\n",
    "    #Clicking on the search button\n",
    "    driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\").click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    ## we will scrap atleast 2 pages product of each items\n",
    "    page_no_url = []\n",
    "    tag= driver.find_elements_by_xpath(\"//nav[@class='yFHi8N']/a\")\n",
    "    for i in tag:\n",
    "        page_no_url.append(i.get_attribute('href'))\n",
    "    for i in page_no_url[0:3]:\n",
    "        driver.get(i)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        ## now appending all products urls from page 1 and page 2:\n",
    "        new_tag = driver.find_elements_by_xpath(\"//a[@class='_1fQZEK']\")\n",
    "        for i in new_tag:\n",
    "            product_urls.append(i.get_attribute('href'))\n",
    "print(len(product_urls))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8e67cfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (198361862.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [23]\u001b[1;36m\u001b[0m\n\u001b[1;33m    iterating all products urls one by one:\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "iterating all products urls one by one:\n",
    "for i in product_urls[:3]:\n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    for _ in range(2):\n",
    "        ## moving cursor deep down on the page\n",
    "        driver.execute_script(\"window.scrollBy(0,6000)\")\n",
    "        time.sleep(1)\n",
    "        \n",
    "    # Clicking on \"see all the reviews\" button:\n",
    "    try:\n",
    "        tag=driver.find_element_by_xpath(\"//div[@class='_2c2kV-']/following::a\")\n",
    "        url = tag.get_attribute('href')\n",
    "        #fetching the url\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    try:\n",
    "        tag=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "        for i in tag:\n",
    "            full_reviews.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        full_reviews.append('-')\n",
    "    try:\n",
    "        tag=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "        for i in tag:\n",
    "            reviews.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        reviews.append('-')\n",
    "    try:\n",
    "        tag=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']/../div\")\n",
    "        for i in tag:\n",
    "            ratings.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        ratings.append('-')\n",
    "               \n",
    "    try:\n",
    "        no_of_pages=driver.find_elements_by_xpath(\"//nav[@class='yFHi8N']/a\")\n",
    "        ListOfNP=[]\n",
    "        for i in no_of_page:\n",
    "            ListOfNP.append(i.get_attribute('href'))\n",
    "        for i in ListOfNP[0:15]:\n",
    "            driver.get(i)\n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                tag=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "                for i in tag:\n",
    "                    full_reviews.append(i.text)\n",
    "            except NoSuchElementException:\n",
    "                full_reviews.append('-')\n",
    "            try:\n",
    "                tag=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "                for i in tag:\n",
    "                    reviews.append(i.text)\n",
    "            except NoSuchElementException:\n",
    "                reviews.append('-')\n",
    "            try:\n",
    "                tag=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']/../div\")\n",
    "                for i in tag:\n",
    "                    ratings.append(i.text)\n",
    "            except NoSuchElementException:\n",
    "                ratings.append('-')    \n",
    "    except: continue\n",
    "len(ratings), len(reviews), len(full_reviews)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfd6fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "df['Ratings']=ratings\n",
    "df['Review_Text']=review_text\n",
    "df['Summary']=title\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ec055",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"E:\\Ratings\\flipkart_ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11e24f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbbb6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6217249",
   "metadata": {},
   "source": [
    "# Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7264f3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating a list of Items, for which we have to scrap their ratings\n",
    "List_of_items = ['laptops','Smartphones','smart watches' , 'Printers','Computer moniters', \n",
    "                 'Digital Watches', 'Router','Cameras','Trimmers','Keyboards']\n",
    "title = []\n",
    "review_text = []\n",
    "ratings = []\n",
    " ## Now we will create a empty list and add all the  item's URLs present in page one.\n",
    "Product_URL=[]\n",
    "\n",
    "## Iterating items one by one in the loop and storing the urls\n",
    "for i in List_of_items:\n",
    "    ## fetching  the website\n",
    "    driver.get(\"https://www.amazon.in/\")\n",
    "    time.sleep(3)\n",
    "    \n",
    "    srchbar = driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "    srchbar.send_keys(i)\n",
    "    \n",
    "    #clicking on search button\n",
    "    \n",
    "    driver.find_element_by_id(\"nav-search-submit-button\").click()  \n",
    "    time.sleep(1)\n",
    "   \n",
    "  \n",
    "    tag=driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\")\n",
    "    for i in tag:\n",
    "        Product_URL.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4e6615",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we create five New empty list for every type of Star rating button's url.\n",
    "Five_star_rating_URL=[]\n",
    "Four_star_rating_URL=[]\n",
    "Three_star_rating_URL=[]\n",
    "Two_star_rating_URL=[]\n",
    "One_star_rating_URL=[]\n",
    "\n",
    "### Iterating all the product URL\n",
    "for i in Product_URL[:250]:\n",
    "    driver.get(i)\n",
    "    time.sleep(1)\n",
    "    for _ in range(2):\n",
    "        ## scolling the cursor at down of the page\n",
    "        driver.execute_script(\"window.scrollBy(0,6000)\")\n",
    "        time.sleep(1)\n",
    "        # Clicking on \"see all reviews\" Button\n",
    "        try:\n",
    "            driver.find_element_by_xpath(\"//a[@class='a-link-emphasis a-text-bold']\").click()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            tag1= driver.find_element_by_xpath(\"//table[@id='histogramTable']/tbody/tr[5]/td[2]/a\")\n",
    "            One_star_rating_URL.append(tag1.get_attribute('href'))\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            tag2 = driver.find_element_by_xpath(\"//table[@id='histogramTable']/tbody/tr[4]/td[2]/a\")\n",
    "            Two_star_rating_URL.append(tag2.get_attribute('href'))\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            tag3 = driver.find_element_by_xpath(\"//table[@id='histogramTable']/tbody/tr[3]/td[2]/a\")\n",
    "            for i in tag3:\n",
    "                Three_star_rating_URL.append(tag3.get_attribute('href'))\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            tag4 = driver.find_element_by_xpath(\"//table[@id='histogramTable']/tbody/tr[2]/td[2]/a\")\n",
    "            Four_star_rating_URL.append(tag4.get_attribute('href'))\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            tag5 = driver.find_element_by_xpath(\"//table[@id='histogramTable']/tbody/tr[1]/td[2]/a\")\n",
    "            Five_star_rating_URL.append(tag5.get_attribute('href'))\n",
    "        except:\n",
    "            pass        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dd5567",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "df['Ratings']=ratings\n",
    "df['Review_Text']=review_text\n",
    "df['Summary']=title\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4c41ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff4c96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
